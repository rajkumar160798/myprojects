@Override
public void createOrReplaceBigQueryTable(String fileName, String datasetName, String tableName) {
    logger.info("Starting BigQuery table creation/replacement....");

    String gcsFilePath = "gs://" + bucketName + "/" + fileName;
    logger.info("GCS File Path: {}", gcsFilePath);
    logger.info("Project ID = {}", projectId);

    try {
        // Preprocess the file to clean up
        Path tempDir = Files.createTempDirectory("cleaned-upload-temp");
        Path tempFilePath = tempDir.resolve("cleaned_" + fileName);

        logger.info("Cleaning the file...");
        preprocessFile(Paths.get(gcsFilePath), tempFilePath);

        // Upload the cleaned file to GCS
        BlobInfo blobInfo = BlobInfo.newBuilder(bucketName, "cleaned_" + fileName).build();
        storage.create(blobInfo, Files.readAllBytes(tempFilePath));
        logger.info("Cleaned file uploaded to GCS: {}/cleaned_{}", bucketName, fileName);

        // Use cleaned file for table creation
        TableId tableId = TableId.of(projectId, datasetName, tableName);
        logger.info("Table ID: = {}", tableId.toString());

        LoadJobConfiguration loadConfig = LoadJobConfiguration.newBuilder(tableId, "gs://" + bucketName + "/cleaned_" + fileName)
                .setFormatOptions(FormatOptions.csv())
                .setAutodetect(true)
                .setWriteDisposition(JobInfo.WriteDisposition.WRITE_TRUNCATE)
                .build();

        String jobName = "jobId_" + UUID.randomUUID().toString();
        JobId jobId = JobId.newBuilder().setLocation("us").setJob(jobName).setProject(computeProjectId)
                .build();

        logger.info("Submitting Bigquery job for table creation: {}", tableName);
        Job job = bigQuery.create(JobInfo.of(jobId, loadConfig));
        job = job.waitFor();

        if (job.isDone()) {
            logger.info("Table created/replaced successfully:{}.{}.{}", projectId, datasetName, tableName);
        } else {
            logger.error("Data load operation failed", job.getStatus().getError());
            throw new RuntimeException("BigQuery create table job failed:" + job.getStatus().getError());
        }

        // Clean up temporary files
        Files.deleteIfExists(tempFilePath);
        Files.deleteIfExists(tempDir);

    } catch (IOException | InterruptedException e) {
        logger.error("Failed to process or upload the file: {}", e.getMessage(), e);
        throw new RuntimeException("BigQuery job failed", e);
    }
}

/**
 * Preprocesses the file to remove empty/null column names and rows with all null values.
 *
 * @param sourceFilePath Path to the source file.
 * @param targetFilePath Path to save the cleaned file.
 */
private void preprocessFile(Path sourceFilePath, Path targetFilePath) throws IOException {
    try (BufferedReader br = Files.newBufferedReader(sourceFilePath, StandardCharsets.UTF_8);
         BufferedWriter bw = Files.newBufferedWriter(targetFilePath, StandardCharsets.UTF_8)) {

        String headerLine = br.readLine();
        if (headerLine == null) {
            throw new IOException("File is empty or missing header.");
        }

        // Filter out null or empty column names
        String[] headers = Arrays.stream(headerLine.split(","))
                                  .map(String::trim)
                                  .filter(header -> header != null && !header.isEmpty())
                                  .toArray(String[]::new);

        // Write cleaned header to the output file
        bw.write(String.join(",", headers));
        bw.newLine();

        // Process the rest of the rows
        String row;
        while ((row = br.readLine()) != null) {
            String[] values = row.split(",");
            if (Arrays.stream(values).anyMatch(value -> value != null && !value.trim().isEmpty())) {
                // Write rows with at least one non-null value
                bw.write(String.join(",", Arrays.copyOfRange(values, 0, headers.length)));
                bw.newLine();
            }
        }
    }
}
